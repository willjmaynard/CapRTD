Implementation
==============

We currently have explored three different implementations for blur detection. Two of these use different types of variance classifiers. The first uses standard image variance with a given threshold. The other calculates the Laplacian transform of the image, calculates the variance of the transform and classifies based on a given threshold. These implementations are mainly meant as baselines to compare other methods against.
Most of our time and effort has been spent on our third implementation which is based on the Deepfocus model. This model is also trained on the data used in the Deepfocus paper. This data set is publicly available and is the backbone of our current work. Here is a brief overview of that set.
The set consists of 204,000 tiles. Each tile is a 64 by 64 resolution color image. These tiles are taken from a variety of different histological slides, including different types of stains. Each tile has a corresponding label from 0 to 10. This label represents how out of focus the tile is. For example, 5 is the ideal focus, 9 is 2 microns over focus, and 1 is 2 microns under focus. These out of focus tiles were imaged with the help of an expert by the Deepfocus authors.
The Deepfocus authors considered a range of -0.5 micron to +0.5 micron to be “in focus”. We adopted this same assumption, so labels 4,5,6 are considered “in focus” and all others are considered “out of focus”. This makes the problem a binary classification problem. Our model is a convolutional neural network based on the one originally presented in the Deepfocus paper, but with a number of changes.
A significant change is a difference in software libraries. The original model uses the tflearn library, while ours was written to use keras with Tensorflow. The convolutional block is as follows – 2 convolutional layers, and 3 alternating blocks of convolution and max pooling. Between each of these layers is a batch normalization layer. The result of the final pooling layer is flattened and connected to the dense block. This has alternating dense and dropout layers with batch normalization. The original Deepfocus model used regularization and dropout, but we could not get that model to converge in our testing. Our model does not have regularization, instead of relying on the dropout layers. Another important difference is the optimizer. The original paper used normal stochastic gradient descent, but we found the optimizer Adam to work better in testing.
Our training methodology is as follows: the first 150,000 images of the dataset were used to train the model and the next 15,000 were used as a test set during training. The rest of the images were reserved as a validation set for the model. Our current best model has ~94% accuracy on the training set and ~89% accuracy on the validation set. So, there is a small amount of overfitting, but the model seems to generalize well.
We have not been able to move forward with implementing a model for tissue fold detection. This is because we still do not have a labeled data set to work with.
